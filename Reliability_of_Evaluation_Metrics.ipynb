{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load your sentiment data\n",
        "def load_sentiment_data(file_path):\n",
        "    # Assumes CSV with 'time' and 'sentiment' columns\n",
        "    data = pd.read_csv(file_path)\n",
        "    # Ensure sentiment is between -1 and 1\n",
        "    data['sentiment_score'] = data['sentiment_score'].clip(-1, 1)\n",
        "    return data\n",
        "\n",
        "# Step 2: Define metrics\n",
        "def MSE(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "def RMSE(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def MAE(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def sRMSE(y_true, y_pred, scale):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    return rmse / scale\n",
        "\n",
        "def MASE(y_true, y_pred, y_naive):\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "    naive_errors = np.abs(y_true - y_naive)\n",
        "    return np.mean(errors) / np.mean(naive_errors[naive_errors != 0] + 1e-10)\n",
        "\n",
        "def sPIS(y_true, y_pred, scale):\n",
        "    errors = y_pred - y_true\n",
        "    return np.mean(errors) / scale  # Mean bias scaled\n",
        "\n",
        "def sAPIS(y_true, y_pred, scale):\n",
        "    errors = y_pred - y_true\n",
        "    return (np.mean(np.abs(errors)) + np.abs(np.mean(errors))) / scale  # Bias + variance\n",
        "\n",
        "def SMAPE(y_true, y_pred):\n",
        "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-10))\n",
        "\n",
        "# Step 3: Test reliability\n",
        "def test_metrics(file_path, n_samples=None):\n",
        "    # Load data\n",
        "    data = load_sentiment_data(file_path)\n",
        "    y_true = data['sentiment_score'].values\n",
        "    if n_samples:\n",
        "        y_true = y_true[:n_samples]  # Limit data size for testing\n",
        "    scale = np.std(y_true) if np.std(y_true) > 0 else 1\n",
        "    # Naive forecast: previous value\n",
        "    y_naive = np.roll(y_true, 1)\n",
        "    y_naive[0] = y_true[0]  # Handle first value\n",
        "\n",
        "    # Define noise parameters\n",
        "    bias_values = [-0.2, 0, 0.2]  # Small bias for -1 to 1 range\n",
        "    variance_values = [0.01, 0.1, 0.5]  # Small variance for sentiment\n",
        "    results = []\n",
        "\n",
        "    # Add noise and compute metrics\n",
        "    for bias in bias_values:\n",
        "        for var in variance_values:\n",
        "            # Add noise and clip to [-1, 1]\n",
        "            noise = np.random.normal(bias, np.sqrt(var), len(y_true))\n",
        "            y_pred = np.clip(y_true + noise, -1, 1)\n",
        "\n",
        "            # Compute metrics\n",
        "            metrics = {\n",
        "                \"bias\": bias,\n",
        "                \"var\": var,\n",
        "                \"MSE\": MSE(y_true, y_pred),\n",
        "                \"RMSE\": RMSE(y_true, y_pred),\n",
        "                \"MAE\": MAE(y_true, y_pred),\n",
        "                \"sRMSE\": sRMSE(y_true, y_pred, scale),\n",
        "                \"MASE\": MASE(y_true, y_pred, y_naive),\n",
        "                \"sPIS\": sPIS(y_true, y_pred, scale),\n",
        "                \"sAPIS\": sAPIS(y_true, y_pred, scale),\n",
        "                \"SMAPE\": SMAPE(y_true, y_pred)\n",
        "            }\n",
        "            results.append(metrics)\n",
        "\n",
        "    # Step 4: Analyze reliability\n",
        "    results_df = pd.DataFrame(results)\n",
        "    reliability = {}\n",
        "    for metric in [\"MSE\", \"RMSE\", \"MAE\", \"sRMSE\", \"MASE\", \"sPIS\", \"sAPIS\", \"SMAPE\"]:\n",
        "        # Variability (standard deviation)\n",
        "        variability = results_df[metric].std()\n",
        "\n",
        "        # Bootstrap confidence intervals\n",
        "        boot_values = []\n",
        "        for _ in range(100):\n",
        "            sample = results_df[metric].sample(frac=1, replace=True)\n",
        "            boot_values.append(sample.mean())\n",
        "        ci = np.percentile(boot_values, [2.5, 97.5])\n",
        "        ci_width = ci[1] - ci[0]\n",
        "\n",
        "        reliability[metric] = {\"variability\": variability, \"ci_width\": ci_width}\n",
        "\n",
        "    # Step 5: Print results\n",
        "    print(\"Metric Reliability (Lower variability and CI width = more reliable):\")\n",
        "    for metric, stats in reliability.items():\n",
        "        print(f\"{metric}: Variability = {stats['variability']:.4f}, CI Width = {stats['ci_width']:.4f}\")\n",
        "\n",
        "    # Plot sensitivity to bias\n",
        "    for metric in [\"MSE\", \"RMSE\", \"MAE\", \"sRMSE\", \"MASE\", \"sPIS\", \"sAPIS\", \"SMAPE\"]:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.scatter(results_df[\"bias\"], results_df[metric], alpha=0.5)\n",
        "        plt.title(f\"{metric} Sensitivity to Bias\")\n",
        "        plt.xlabel(\"Bias\")\n",
        "        plt.ylabel(metric)\n",
        "        plt.savefig(f\"{metric}_sensitivity.png\")\n",
        "        plt.close()\n",
        "\n",
        "    return results_df, reliability\n",
        "\n",
        "# Run the test\n",
        "# Replace 'your_data.csv' with the path to your CSV file\n",
        "results_df, reliability = test_metrics('/content/Score_output_data.csv', n_samples=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjiM04UKsmmt",
        "outputId": "788f34c6-b1a9-4ba0-dcdc-5129a4a81c24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric Reliability (Lower variability and CI width = more reliable):\n",
            "MSE: Variability = 0.1296, CI Width = 0.1544\n",
            "RMSE: Variability = 0.1747, CI Width = 0.2173\n",
            "MAE: Variability = 0.1333, CI Width = 0.1537\n",
            "sRMSE: Variability = 0.3247, CI Width = 0.4015\n",
            "MASE: Variability = 0.3112, CI Width = 0.3709\n",
            "sPIS: Variability = 0.2676, CI Width = 0.3380\n",
            "sAPIS: Variability = 0.3014, CI Width = 0.3389\n",
            "SMAPE: Variability = 23.1660, CI Width = 26.1985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fU6tG4H9suqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}